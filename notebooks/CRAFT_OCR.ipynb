{"cells":[{"cell_type":"markdown","source":["# Installing the necessary Python packages and importing essential libraries required for OCR, Face detection, text processing, and related tasks"],"metadata":{"id":"KKDibP4v27t9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Omc97Y8dNDJZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757215144693,"user_tz":-60,"elapsed":32992,"user":{"displayName":"Mourad Boutrid","userId":"11848014835914515491"}},"outputId":"187b4de8-f80c-4a27-8690-86890b34775e"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.4/288.4 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m122.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m963.8/963.8 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install python-doctr -q\n","!pip install ultralytics  -q\n","!pip install Levenshtein -q"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jF81ABmNSel3","outputId":"3f853308-e21a-4b93-fa3e-a774e5d38de9","executionInfo":{"status":"ok","timestamp":1757215190987,"user_tz":-60,"elapsed":46291,"user":{"displayName":"Mourad Boutrid","userId":"11848014835914515491"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nOqQVjxpPPYy"},"outputs":[],"source":["import sys\n","import os\n","import cv2\n","import numpy as np\n","import unicodedata\n","import string\n","\n","from collections import OrderedDict\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from PIL import Image, ImageEnhance, ImageFilter\n","\n","import re\n","import torch\n","from datetime import datetime\n","import json\n","import difflib\n","\n","from doctr.models import ocr_predictor\n","from doctr.io import DocumentFile\n","\n","import torch.nn as nn\n","from torchvision import transforms"]},{"cell_type":"markdown","source":["# Defining Mean Functions for OCR and Text Classification"],"metadata":{"id":"_hkoK7Op3C5x"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Ej8hIJtMdob"},"outputs":[],"source":["def extract_coordinates(geometry):\n","    if hasattr(geometry, '__len__') and len(geometry) == 2:\n","        top_left, bottom_right = geometry\n","        x1, y1 = top_left\n","        x2, y2 = bottom_right\n","        coordinates = [\n","            [x1, y1],\n","            [x2, y1],\n","            [x2, y2],\n","            [x1, y2]\n","        ]\n","        return coordinates\n","    return []\n","\n","def load_model(model_path, config_path):\n","    with open(config_path, 'r') as f:\n","        config = json.load(f)\n","\n","    predictor = ocr_predictor(\n","        det_arch=config['det_arch'],\n","        reco_arch=config['reco_arch'],\n","        pretrained=False\n","    )\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    predictor.load_state_dict(torch.load(model_path, map_location=device))\n","    predictor.eval()\n","\n","    if torch.cuda.is_available():\n","        predictor = predictor.cuda()\n","\n","    return predictor\n","\n","def detect_text(predictor, image_path, text_threshold=0.7):\n","    doc = DocumentFile.from_images(image_path)\n","\n","    with torch.no_grad():\n","        result = predictor(doc)\n","\n","    boxes = []\n","    for page in result.pages:\n","        for block in page.blocks:\n","            for line in block.lines:\n","                coordinates = extract_coordinates(line.geometry)\n","\n","                if len(coordinates) == 4:\n","                    box = np.array(coordinates)\n","\n","                    if hasattr(line, 'confidence') and line.confidence >= text_threshold:\n","                        boxes.append(box)\n","                    elif not hasattr(line, 'confidence'):\n","                        boxes.append(box)\n","\n","    image = cv2.imread(image_path)\n","    height, width = image.shape[:2]\n","\n","    pixel_boxes = []\n","    for box in boxes:\n","        pixel_box = box.copy()\n","        pixel_box[:, 0] *= width\n","        pixel_box[:, 1] *= height\n","        pixel_boxes.append(pixel_box)\n","\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","    return image, np.array(pixel_boxes), np.array(pixel_boxes)\n","\n","def show_boxes(image, boxes):\n","    img_show = image.copy()\n","    for box in boxes:\n","        box = box.astype(np.int32).reshape((-1, 1, 2))\n","        cv2.polylines(img_show, [box], isClosed=True, color=(0, 255, 0), thickness=2)\n","    plt.figure(figsize=(12, 12))\n","    plt.imshow(img_show)\n","    plt.axis('off')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qNsxIwe9TnUW"},"outputs":[],"source":["def crop_id_card(image, boxes):\n","\n","    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    crops = []\n","\n","    for poly in boxes:\n","        poly = np.array(poly).astype(np.int32)\n","\n","        # Create mask\n","        mask = np.zeros(image.shape[:2], dtype=np.uint8)\n","        cv2.fillPoly(mask, [poly], 255)\n","\n","        # Apply mask\n","        masked = cv2.bitwise_and(image_rgb, image_rgb, mask=mask)\n","\n","        # Get bounding box and crop\n","        x, y, w, h = cv2.boundingRect(poly)\n","        cropped = masked[y:y+h, x:x+w]\n","        crops.append(cropped)\n","\n","    return crops\n","\n","\n","def plot_crops(crops, per_row=5, size=4):\n","\n","    n = len(crops)\n","    rows = (n + per_row - 1) // per_row  # Ceiling division\n","\n","    fig, axs = plt.subplots(rows, per_row, figsize=(per_row * size, rows * size))\n","\n","    # Flatten axs for easy indexing, handle 1D or 2D array of axes\n","    axs = axs.flatten() if isinstance(axs, np.ndarray) else [axs]\n","\n","    for i in range(len(axs)):\n","        if i < n:\n","            axs[i].imshow(crops[i])\n","            axs[i].set_title(f\"Crop #{i+1}\")\n","        axs[i].axis('off')\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xir2pZfmTrC7"},"outputs":[],"source":["# Define the classifier saved model architecture\n","class LanguageClassifier(nn.Module):\n","    def __init__(self, num_classes):\n","        super(LanguageClassifier, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","        )\n","\n","        # Classifier\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(0.5),\n","            nn.Linear(256 * 4 * 16, 512),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","            nn.Linear(512, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.classifier(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NGTs8oinUDO-"},"outputs":[],"source":["class_names = ['Arabic', 'Frensh']\n","\n","def predict_language_cnn(image, model, class_names, transform):\n","    if isinstance(image, np.ndarray):\n","        image = Image.fromarray(image)\n","\n","    image_tensor = transform(image).unsqueeze(0)\n","\n","    with torch.no_grad():\n","        output = model(image_tensor)\n","        probabilities = torch.nn.functional.softmax(output, dim=1)\n","        confidence, predicted = torch.max(probabilities, 1)\n","\n","    predicted_class = class_names[predicted.item()]\n","    confidence = confidence.item() * 100\n","\n","    return predicted_class, confidence\n","\n","def classify_crops(crops):\n","    results = []\n","\n","    for i, crop in enumerate(crops):\n","        try:\n","            prediction, confidence = predict_language_cnn(crop, model, class_names, test_transform)\n","            results.append((prediction, confidence))\n","            print(f\"Crop {i+1}: {prediction} ({confidence:.2f}%)\")\n","\n","        except Exception as e:\n","            print(f\"Error processing crop {i+1}: {str(e)}\")\n","            results.append((\"Error\", 0))\n","\n","    return results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mszsjpz4VigM"},"outputs":[],"source":["def plot_classification(original_image, crops, boxes, classification_results):\n","    img_with_boxes = original_image.copy()\n","\n","    for i, (box, (prediction, confidence)) in enumerate(zip(boxes, classification_results)):\n","        box = box.astype(np.int32).reshape((-1, 1, 2))\n","\n","        # Colors: Green for Arabic, Blue for English, Red for errors\n","        color = (0, 255, 0) if prediction == 'Arabic' else (255, 0, 0) if prediction == 'Frensh' else (0, 0, 255)\n","\n","        # Draw thicker box\n","        cv2.polylines(img_with_boxes, [box], isClosed=True, color=color, thickness=3)\n","\n","        # Get top-left corner for text placement\n","        x_min = np.min(box[:, 0, 0])\n","        y_min = np.min(box[:, 0, 1])\n","\n","        # Create background for text\n","        label = f\"{prediction} {confidence:.0f}%\"\n","        text_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]\n","\n","        # Draw text background\n","        cv2.rectangle(img_with_boxes,\n","                     (x_min, y_min - text_size[1] - 5),\n","                     (x_min + text_size[0] + 10, y_min),\n","                     color, -1)\n","\n","        # Draw text\n","        cv2.putText(img_with_boxes, label, (x_min + 5, y_min - 5),\n","                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n","\n","    plt.figure(figsize=(15, 10))\n","    plt.imshow(cv2.cvtColor(img_with_boxes, cv2.COLOR_BGR2RGB))\n","    plt.axis('off')\n","    plt.title('Language Classification Results')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kUGxDBygXpU7"},"outputs":[],"source":["def run_ocr(image):\n","    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","    with torch.no_grad():\n","        result = predictor([image_rgb])\n","\n","    ocr_results = []\n","    for page in result.pages:\n","        for block in page.blocks:\n","            for line in block.lines:\n","                line_text = \" \".join([word.value for word in line.words])\n","\n","                if line.words:\n","                    x_coords = []\n","                    y_coords = []\n","                    for word in line.words:\n","                        for point in word.geometry:\n","                            x_coords.append(point[0])\n","                            y_coords.append(point[1])\n","\n","                    x_min, x_max = min(x_coords), max(x_coords)\n","                    y_min, y_max = min(y_coords), max(y_coords)\n","\n","                    ocr_results.append({\n","                        'text': line_text,\n","                        'bbox': (x_min, y_min, x_max, y_max),\n","                        'confidence': sum(word.confidence for word in line.words) / len(line.words)\n","                    })\n","\n","    return ocr_results\n","\n","def plot_ocr_results(image, ocr_results):\n","    img_with_text = image.copy()\n","    height, width = image.shape[:2]\n","\n","    for i, result in enumerate(ocr_results):\n","        text = result['text']\n","        confidence = result['confidence']\n","        x_min, y_min, x_max, y_max = result['bbox']\n","\n","        # Convert normalized coordinates to pixels\n","        x_min_px = int(x_min * width)\n","        y_min_px = int(y_min * height)\n","        x_max_px = int(x_max * width)\n","        y_max_px = int(y_max * height)\n","\n","        # Draw bounding box\n","        cv2.rectangle(img_with_text, (x_min_px, y_min_px), (x_max_px, y_max_px), (0, 255, 255), 2)\n","\n","        # Draw text background\n","        label = f\"{text} ({confidence:.2f})\"\n","        text_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]\n","\n","        cv2.rectangle(img_with_text,\n","                     (x_min_px, y_min_px - text_size[1] - 5),\n","                     (x_min_px + text_size[0] + 10, y_min_px),\n","                     (0, 255, 255), -1)\n","\n","        # Draw text\n","        cv2.putText(img_with_text, label, (x_min_px + 5, y_min_px - 5),\n","                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n","\n","    plt.figure(figsize=(15, 10))\n","    plt.imshow(cv2.cvtColor(img_with_text, cv2.COLOR_BGR2RGB))\n","    plt.axis('off')\n","    plt.title('OCR Results - All Detected Text')\n","    plt.show()\n","\n","def display_ocr_results(ocr_results):\n","    print(\"=== OCR RESULTS ===\")\n","    print(f\"Found {len(ocr_results)} text lines:\")\n","    print(\"-\" * 50)\n","\n","    for i, result in enumerate(ocr_results, 1):\n","        print(f\"{i}. '{result['text']}'\")\n","        print(f\"   Confidence: {result['confidence']:.3f}\")\n","        print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SyaD7CSjeKkH"},"outputs":[],"source":["def filter_ocr_with_classification(ocr_results, image, model, class_names, transform, confidence_threshold=0.7):\n","    filtered_texts = []\n","    height, width = image.shape[:2]\n","\n","    for i, ocr_item in enumerate(ocr_results):\n","        text = ocr_item['text']\n","        ocr_confidence = ocr_item['confidence']\n","        x_min, y_min, x_max, y_max = ocr_item['bbox']\n","\n","        # Skip empty text\n","        if not text.strip():\n","            continue\n","\n","        # Convert normalized coordinates to pixels\n","        x_min_px = int(x_min * width)\n","        y_min_px = int(y_min * height)\n","        x_max_px = int(x_max * width)\n","        y_max_px = int(y_max * height)\n","\n","        # Extract the region from image\n","        region = image[y_min_px:y_max_px, x_min_px:x_max_px]\n","\n","        if region.size == 0:\n","            continue\n","\n","        # Classify the region\n","        predicted_class, classification_confidence = predict_language_cnn(region, model, class_names, transform)\n","\n","        # Keep text based on classification rules\n","        if ocr_confidence < confidence_threshold:\n","            # Low OCR confidence - use classification to decide\n","            if predicted_class == 'Arabic' and classification_confidence > 70:\n","                # Low confidence OCR + high confidence Arabic classification = discard\n","                print(f\"Discarding: '{text}' (OCR: {ocr_confidence:.3f}, Class: {predicted_class} {classification_confidence:.1f}%)\")\n","                continue\n","            else:\n","                # Keep if French or uncertain\n","                filtered_texts.append({\n","                    'text': text,\n","                    'ocr_confidence': ocr_confidence,\n","                    'classification': predicted_class,\n","                    'classification_confidence': classification_confidence,\n","                    'bbox': ocr_item['bbox'],\n","                    'region_id': i + 1\n","                })\n","        else:\n","            # High OCR confidence - keep regardless of classification\n","            filtered_texts.append({\n","                'text': text,\n","                'ocr_confidence': ocr_confidence,\n","                'classification': predicted_class,\n","                'classification_confidence': classification_confidence,\n","                'bbox': ocr_item['bbox'],\n","                'region_id': i + 1\n","            })\n","\n","    return filtered_texts\n","\n","def display_filtered_ocr_results(filtered_texts):\n","    print(\"=== FILTERED OCR RESULTS ===\")\n","    print(f\"Total texts: {len(filtered_texts)}\")\n","    print(\"-\" * 50)\n","\n","    french_texts = [item for item in filtered_texts if item['classification'] == 'French']\n","    arabic_texts = [item for item in filtered_texts if item['classification'] == 'Arabic']\n","    other_texts = [item for item in filtered_texts if item['classification'] not in ['French', 'Arabic']]\n","\n","    print(f\"French texts: {len(french_texts)}\")\n","    print(f\"Arabic texts: {len(arabic_texts)}\")\n","    print(f\"Other/Uncertain: {len(other_texts)}\")\n","    print(\"-\" * 50)\n","\n","    if french_texts:\n","        print(\"\\n FRENCH TEXT (KEEP):\")\n","        for result in french_texts:\n","            print(f\"Region {result['region_id']}: '{result['text']}'\")\n","            print(f\"  OCR confidence: {result['ocr_confidence']:.3f}\")\n","            print(f\"  Classification: {result['classification']} ({result['classification_confidence']:.1f}%)\")\n","            print()\n","\n","    if arabic_texts:\n","        print(\"\\n ARABIC TEXT (FILTERED):\")\n","        for result in arabic_texts:\n","            print(f\"Region {result['region_id']}: '{result['text']}'\")\n","            print(f\"  OCR confidence: {result['ocr_confidence']:.3f}\")\n","            print(f\"  Classification: {result['classification']} ({result['classification_confidence']:.1f}%)\")\n","            print()\n","\n","    if other_texts:\n","        print(\"\\n OTHER TEXT (KEEP):\")\n","        for result in other_texts:\n","            print(f\"Region {result['region_id']}: '{result['text']}'\")\n","            print(f\"  OCR confidence: {result['ocr_confidence']:.3f}\")\n","            print(f\"  Classification: {result['classification']} ({result['classification_confidence']:.1f}%)\")\n","            print()\n","\n","def plot_final_results(image, filtered_texts):\n","    img_with_text = image.copy()\n","    height, width = image.shape[:2]\n","\n","    for result in filtered_texts:\n","        text = result['text']\n","        x_min, y_min, x_max, y_max = result['bbox']\n","        language = result['classification']\n","\n","        x_min_px = int(x_min * width)\n","        y_min_px = int(y_min * height)\n","        x_max_px = int(x_max * width)\n","        y_max_px = int(y_max * height)\n","\n","        # Color based on language\n","        color = (0, 0, 255) if language == 'French' else (0, 255, 0)  # Red for French, Green for others\n","\n","        cv2.rectangle(img_with_text, (x_min_px, y_min_px), (x_max_px, y_max_px), color, 2)\n","\n","        label = f\"{language}: {text}\"\n","        text_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]\n","\n","        cv2.rectangle(img_with_text,\n","                     (x_min_px, y_min_px - text_size[1] - 5),\n","                     (x_min_px + text_size[0] + 10, y_min_px),\n","                     color, -1)\n","\n","        cv2.putText(img_with_text, label, (x_min_px + 5, y_min_px - 5),\n","                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n","\n","    plt.figure(figsize=(15, 10))\n","    plt.imshow(cv2.cvtColor(img_with_text, cv2.COLOR_BGR2RGB))\n","    plt.axis('off')\n","    plt.title('Final Filtered OCR Results')\n","    plt.show()\n"]},{"cell_type":"markdown","source":["# Defining Mean Functions for Extracting Information from Front Side of ID Cards"],"metadata":{"id":"wNcjC2Ac3a8E"}},{"cell_type":"code","source":["def find_name_blocks_idf(blocks):\n","    target_words = {\"carte\", \"nationale\", \"identite\"}\n","    header_block = None\n","\n","    # Find header block containing any target words\n","    for block in blocks:\n","        text = block.get('text', '').lower()\n","        if any(word in text for word in target_words):\n","            header_block = block\n","            break\n","    if not header_block:\n","        return None, None\n","\n","    # Calculate full horizontal range across all blocks\n","    all_x_mins = [b['bbox'][0] for b in blocks]\n","    all_x_maxs = [b['bbox'][2] for b in blocks]\n","    min_x = min(all_x_mins)\n","    max_x = max(all_x_maxs)\n","    horizontal_mid = (min_x + max_x) / 2\n","\n","    hx_min, hy_min, hx_max, hy_max = header_block['bbox']\n","\n","    # Filter candidate blocks:\n","    # - Vertically below the header (bbox[1] > header's bottom)\n","    # - Horizontally within the left half (bbox[0] < horizontal midpoint)\n","    candidates = [b for b in blocks\n","                  if b['bbox'][1] > hy_max and\n","                  b['bbox'][0] < horizontal_mid]\n","\n","    # Sort candidates top-to-bottom by their y-min\n","    candidates.sort(key=lambda b: b['bbox'][1])\n","\n","    if len(candidates) < 2:\n","        return None, None\n","\n","    prenom = candidates[0]['text']\n","    nom = candidates[1]['text']\n","\n","    return prenom, nom\n","\n","\n","def extract_cin_from_blocks_idf(blocks):\n","    pattern = re.compile(r'\\b[A-Z]{1,3}\\d{4,10}\\b')\n","    results = []\n","    for block in blocks:\n","        text = block.get('text', '').upper()\n","        matches = pattern.findall(text)\n","        results.extend(matches)\n","    return results\n","\n","\n","def extract_dates_from_ocr_blocks_idf(blocks):\n","    date_pattern = re.compile(r'\\b(\\d{2})[\\s./-](\\d{2})[\\s./-](\\d{4})\\b')\n","    year_pattern = re.compile(r'\\b(19\\d{2})\\b')\n","\n","    result = {\n","        \"date_de_naissance\": None,\n","        \"valable_jusqua\": None\n","    }\n","\n","    texts = [block['text'] for block in blocks]\n","    found_dates = []\n","\n","    for text in texts:\n","        matches = date_pattern.findall(text)\n","        raw_matches = date_pattern.finditer(text)\n","        for match in raw_matches:\n","            full_raw_date = match.group(0)  # e.g. '01.02.1980'\n","            found_dates.append(full_raw_date)\n","        if len(found_dates) >= 2:\n","            break\n","\n","    if len(found_dates) >= 2:\n","        result[\"date_de_naissance\"] = found_dates[0]\n","        result[\"valable_jusqua\"] = found_dates[1]\n","        return result\n","\n","    if len(found_dates) == 1:\n","        result[\"valable_jusqua\"] = found_dates[0]\n","\n","        for text in texts:\n","            clean_text = re.sub(r'[^\\w\\s]', '', text)\n","            year_matches = year_pattern.findall(clean_text)\n","            for y in year_matches:\n","                if y != found_dates[0][-4:]:\n","                    result[\"date_de_naissance\"] = y\n","                    return result\n","        return result\n","\n","    for text in texts:\n","        clean_text = re.sub(r'[^\\w\\s]', '', text)\n","        year_matches = year_pattern.findall(clean_text)\n","        if year_matches:\n","            result[\"date_de_naissance\"] = year_matches[0]\n","            break\n","\n","    return result\n","\n","\n","\n","def normalize_text_idf(text):\n","    return re.sub(r'[^a-z]', '', text.lower())\n","\n","def similarity_ratio_idf(a, b):\n","    a_norm = normalize_text_idf(a)\n","    b_norm = normalize_text_idf(b)\n","    if not a_norm or not b_norm:\n","        return 0\n","    matches = sum(1 for x, y in zip(a_norm, b_norm) if x == y)\n","    return matches / max(len(a_norm), len(b_norm))\n","\n","def convert_date_to_iso_idf(date_str):\n","    date_pattern = re.compile(r'(\\d{2})[\\s./-](\\d{2})[\\s./-](\\d{4})')\n","    m = date_pattern.search(date_str)\n","    if m:\n","        day, month, year = m.groups()\n","        try:\n","            return datetime.strptime(f'{day}-{month}-{year}', '%d-%m-%Y').date().isoformat()\n","        except:\n","            return None\n","    return None\n","\n","def clean_ocr_blocks_idf(blocks):\n","    unwanted_texts = ['royaume du maroc', 'royaume maroc', 'carte nationale didentite',\n","                      \"carte nationale d'identite\", 'carte nationale identite', 'carte national didentite',\n","                      \"carte national d'identite\", 'carte national identite', 'specmen', 'specimen', 'ne le',\n","                      'née le', 'neé le', 'nee le', 'néle', 'néé le', 'nééle', 'neéle', 'du maroc', 'royaume du',\n","                      'nationale didentite', \"nationale d'identite\", 'nationale identite', 'national didentite',\n","                      \"national d'identite\", 'national identite', 'carte didentite', \"carte d'identite\", 'carte identite',\n","                      \"maroc\", \"auime du maroc\",\"N\\\"\", 'Valable jusqu\\'au', \"royaume\", \"ROYAUUE DU MADOO\"\n","                      ]\n","\n","\n","\n","    dates = extract_dates_from_ocr_blocks_idf(blocks)\n","    dates_to_remove = {v for v in dates.values() if v is not None}\n","\n","    cin_list = extract_cin_from_blocks_idf(blocks)\n","    cin_set = set(cin_list)\n","\n","    cleaned_blocks = []\n","    for block in blocks:\n","        text = block.get('text', '').strip()\n","        if len(text) <= 1:\n","            continue\n","\n","        if any(similarity_ratio_idf(text, unwanted) >= 0.9 for unwanted in unwanted_texts):\n","            continue\n","\n","        # Check dates\n","        date_substrings = re.findall(r'\\b\\d{2}[\\s./-]\\d{2}[\\s./-]\\d{4}\\b', text)\n","        block_dates = set()\n","        for ds in date_substrings:\n","            iso_date = convert_date_to_iso_idf(ds)\n","            if iso_date:\n","                block_dates.add(iso_date)\n","\n","        if block_dates.intersection(dates_to_remove):\n","            continue\n","\n","        # Check CIN pattern exact matches in this text (ignore case)\n","        text_upper = text.upper()\n","        if any(cin in text_upper for cin in cin_set):\n","            continue\n","\n","        cleaned_blocks.append(block)\n","\n","    return cleaned_blocks\n","\n","\n","def extract_names_with_validation_idf(blocks):\n","    cleaned_blocks = clean_ocr_blocks_idf(blocks)\n","    prenom, nom = find_name_blocks_idf(blocks)\n","\n","    if prenom is None or nom is None:\n","        return None, None\n","\n","    cleaned_texts = [block['text'].lower() for block in cleaned_blocks]\n","\n","    def text_present(text):\n","        norm_text = normalize_text_idf(text)\n","        return any(norm_text in normalize_text_idf(ct) for ct in cleaned_texts)\n","\n","    if not text_present(prenom) or not text_present(nom):\n","        return None, None\n","\n","    return prenom, nom\n","\n","def remove_name_blocks_idf(blocks):\n","\n","    prenom, nom = extract_names_with_validation_idf(filtered_texts)\n","    prenom_norm = normalize_text_idf(prenom)\n","    nom_norm = normalize_text_idf(nom)\n","\n","    filtered_blocks = []\n","    for block in blocks:\n","        text_norm = normalize_text_idf(block.get('text', ''))\n","        if text_norm == prenom_norm or text_norm == nom_norm:\n","            continue\n","        filtered_blocks.append(block)\n","\n","    return filtered_blocks\n","\n","\n","\n","\n","def extract_lieu_idf(raw_blocks, clean_blocks):\n","    dates = extract_dates_from_ocr_blocks_idf(raw_blocks)\n","    ref_block = None\n","\n","    # Try to find 'ne le' or similar in raw_blocks as reference block\n","    for block in raw_blocks:\n","        text_lower = block['text'].lower()\n","        if any(phrase in text_lower for phrase in ['neele', 'nele', 'ne le', 'né le', 'née le', 'ne lé', 'né lé', 'née lé', 'ne l', 'né l', 'née l', 'ne le.', 'né le.', 'née le.', 'ne-le', 'né-le', 'née-le', 'nè le', 'nè lé', 'nè-lé', 'nè-le', 'ne lè', 'né lè', 'née lè']):\n","            ref_block = block\n","            break\n","\n","    # If not found, try to find block matching date_de_naissance from extracted dates\n","    if not ref_block and dates['date_de_naissance']:\n","        date_str = dates['date_de_naissance'].replace('-', '.')\n","        for block in raw_blocks:\n","            if date_str in block['text']:\n","                ref_block = block\n","                break\n","\n","    if not ref_block:\n","        return None\n","\n","    ref_x_min, ref_y_min, ref_x_max, ref_y_max = ref_block['bbox']\n","\n","    candidates = [b for b in clean_blocks\n","                  if b['bbox'][0] <= ref_x_min + 0.05  # left aligned within 5%\n","                  and b['bbox'][1] > ref_y_max]        # below reference block\n","\n","    if not candidates:\n","        return None\n","\n","    candidates.sort(key=lambda b: b['bbox'][1])\n","\n","    first_line = candidates[0]\n","    first_text = first_line['text']\n","\n","    if len(candidates) > 1:\n","        second_line = candidates[1]\n","\n","        vertical_gap = second_line['bbox'][1] - first_line['bbox'][3]\n","        horizontal_gap = abs(second_line['bbox'][0] - first_line['bbox'][0])\n","\n","        if vertical_gap < 0.03 and horizontal_gap < 0.03 and len(first_text) > 5:\n","            return first_text + ' ' + second_line['text']\n","\n","    return first_text\n","\n","\n","\n","\n","def extract_id_info(blocks):\n","\n","    cin = extract_cin_from_blocks_idf(blocks)\n","    dates = extract_dates_from_ocr_blocks_idf(blocks)\n","\n","    clean = clean_ocr_blocks_idf(blocks)\n","    prenom, nom = extract_names_with_validation_idf(blocks)\n","    clean = remove_name_blocks_idf(clean)\n","\n","    lieu = extract_lieu_idf(blocks, clean)\n","\n","    result = {\n","        \"pays\": \"Maroc\",\n","        \"type_de_carte\": \"Carte Nationale d'Identité\",\n","        \"prenom\": prenom,\n","        \"Nom\": nom,\n","        \"date_de_naissance\": dates.get(\"date_de_naissance\"),\n","        \"valable_jusqua\": dates.get(\"valable_jusqua\"),\n","        \"lieu_de_naissance\": lieu,\n","        \"CIN\": (cin[0] if len(cin) != 0 else None)\n","    }\n","\n","    return result\n"],"metadata":{"id":"KdYn2piU8-iJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Defining Mean Functions for Extracting Information from Back Side of ID Cards"],"metadata":{"id":"6us3GJw93zli"}},{"cell_type":"code","source":["def extract_4_info_idb(blocks):\n","    cin_pattern = re.compile(r'[A-Z]{1,3}\\d{4,10}')\n","    code_pattern = re.compile(r'^[A-Z0-9]{7,10}$')\n","    num_etat_pattern = re.compile(r'\\b\\d+(?:/\\d+)+\\b')\n","\n","    cin_candidates = []\n","    code_candidates = []\n","\n","    for block in blocks:\n","        raw_text = block.get('text', '').upper()\n","        bbox = block.get('bbox', [])\n","        if not bbox or len(bbox) != 4:\n","            continue\n","\n","        cin_matches = cin_pattern.findall(raw_text)\n","        for cin_match in cin_matches:\n","            if len(cin_match) < 9:\n","                cin_candidates.append((bbox, cin_match))\n","\n","        text_stripped = raw_text.strip()\n","        if len(text_stripped) < 11 and code_pattern.fullmatch(text_stripped):\n","            code_candidates.append((bbox, text_stripped))\n","\n","    if cin_candidates:\n","        cin_candidates.sort(key=lambda item: (item[0][1], item[0][0]))\n","        cin_result = [cin_candidates[0][1]]\n","        cin_bbox = cin_candidates[0][0]\n","    else:\n","        cin_result = []\n","        cin_bbox = None\n","\n","    if code_candidates:\n","        code_candidates.sort(key=lambda item: (item[0][1], -item[0][0]))\n","        code_result = [code_candidates[-1][1]]\n","        code_bbox = code_candidates[-1][0]\n","    else:\n","        code_result = []\n","        code_bbox = None\n","\n","    num_etat_civil_results = []\n","    if cin_bbox and code_bbox:\n","        left_x = cin_bbox[2]\n","        right_x = code_bbox[0]\n","        max_y = max(cin_bbox[1], code_bbox[1])\n","\n","        candidates = []\n","        for block in blocks:\n","            bbox = block.get('bbox', [])\n","            if not bbox or len(bbox) != 4:\n","                continue\n","            x_min, y_min, _, _ = bbox\n","            if left_x <= x_min <= right_x and y_min <= max_y + 0.05:\n","                text = block.get('text', '').strip()\n","                matches = num_etat_pattern.findall(text)\n","                for match in matches:\n","                    candidates.append((bbox, match))\n","\n","        if candidates:\n","            if len(candidates) == 1:\n","                num_etat_civil_results = [candidates[0][1]]\n","            else:\n","                center_x = (left_x + right_x) / 2\n","                center_y = max_y / 2\n","\n","                def distance_to_center(bbox):\n","                    x_min, y_min, x_max, y_max = bbox\n","                    box_center_x = (x_min + x_max) / 2\n","                    box_center_y = (y_min + y_max) / 2\n","                    return ((box_center_x - center_x) ** 2 + (box_center_y - center_y) ** 2) ** 0.5\n","\n","                candidates.sort(key=lambda c: distance_to_center(c[0]))\n","                num_etat_civil_results = [candidates[0][1]]\n","\n","    for block in blocks:\n","        text = block.get('text', '').strip().upper()\n","        if re.fullmatch(r'SEXE\\s*M', text):\n","            gender = 'M'\n","            break\n","        if re.fullmatch(r'SEXE\\s*F', text):\n","            gender = 'F'\n","            break\n","    else:\n","        gender = None\n","\n","    if gender is None and cin_bbox:\n","        cin_x_min, cin_y_min, cin_x_max, cin_y_max = cin_bbox\n","        for block in blocks:\n","            bbox = block.get('bbox', [])\n","            if not bbox or len(bbox) != 4:\n","                continue\n","            conf = block.get('conf', 0)\n","            if conf <= 0.8:\n","                continue\n","            x_min, y_min, x_max, y_max = bbox\n","            text = block.get('text', '').strip().upper()\n","\n","            horizontally_aligned = (cin_x_min <= x_min <= cin_x_max) or (cin_x_min <= x_max <= cin_x_max)\n","            vertically_below = y_min > cin_y_max\n","            if horizontally_aligned and vertically_below:\n","                if text.startswith('FILE DE') or text.startswith('FILEDE'):\n","                    gender = 'F'\n","                    break\n","                if text.startswith('FILS DE') or text.startswith('FILSDE'):\n","                    gender = 'Male'\n","                    break\n","\n","    if gender is None and code_bbox:\n","        code_x_min, code_y_min, code_x_max, code_y_max = code_bbox\n","        for block in blocks:\n","            bbox = block.get('bbox', [])\n","            if not bbox or len(bbox) != 4:\n","                continue\n","            x_min, y_min, x_max, y_max = bbox\n","            text = block.get('text', '').strip().upper()\n","            horizontally_aligned = (code_x_min <= x_min <= code_x_max) or (code_x_min <= x_max <= code_x_max)\n","            vertically_below = y_min > code_y_max\n","            if horizontally_aligned and vertically_below and len(text) <= 6:\n","                if re.search(r'\\bSEXE\\s*M\\b', text):\n","                    gender = 'M'\n","                    break\n","                if re.search(r'\\bSEXE\\s*F\\b', text):\n","                    gender = 'F'\n","                    break\n","\n","        if gender is None:\n","            for block in blocks:\n","                bbox = block.get('bbox', [])\n","                if not bbox or len(bbox) != 4:\n","                    continue\n","                x_min, y_min, x_max, y_max = bbox\n","                text = block.get('text', '').strip().upper()\n","                horizontally_aligned = (code_x_min <= x_min <= code_x_max) or (code_x_min <= x_max <= code_x_max)\n","                vertically_below = y_min > code_y_max\n","                if horizontally_aligned and vertically_below and len(text) <= 6:\n","                    if re.search(r'\\bM\\b', text):\n","                        gender = 'Male'\n","                        break\n","                    if re.search(r'\\bF\\b', text):\n","                        gender = 'Female'\n","                        break\n","\n","    return cin_result, code_result, num_etat_civil_results, gender\n","\n","def extract_address_from_blocks_idb(blocks):\n","    address_keywords = {'ADRESSE', 'ADRESS', 'ADRES', 'ADESSE'}\n","\n","    # First pass: Look for \"adresse-like\" word in the first two words of the block\n","    for block in blocks:\n","        text = block.get('text', '').upper().strip()\n","        words = re.split(r'\\s+', text)\n","        first_two = words[:2]\n","        if any(word in address_keywords for word in first_two):\n","            return text\n","\n","    # Fallback: Positional logic using CIN\n","    cin_pattern = re.compile(r'[A-Z]{1,3}\\d{4,10}')\n","    cin_candidates = []\n","\n","    for block in blocks:\n","        text = block.get('text', '').upper()\n","        bbox = block.get('bbox', [])\n","        if len(bbox) != 4:\n","            continue\n","        cin_matches = cin_pattern.findall(text)\n","        for match in cin_matches:\n","            cin_candidates.append((bbox, match))\n","\n","    if not cin_candidates:\n","        return None\n","\n","    # Get top-left-most CIN block\n","    cin_candidates.sort(key=lambda item: (item[0][1], item[0][0]))\n","    cin_bbox = cin_candidates[0][0]\n","    cin_x_min, _, _, cin_y_max = cin_bbox\n","\n","    # Find blocks that are below CIN and roughly horizontally aligned\n","    aligned_blocks = []\n","    for block in blocks:\n","        bbox = block.get('bbox', [])\n","        if len(bbox) != 4:\n","            continue\n","        x_min, y_min, _, _ = bbox\n","        if abs(x_min - cin_x_min) < 0.1 and y_min > cin_y_max:\n","            aligned_blocks.append((y_min, block))\n","\n","    # Sort by vertical position and take the third block (index 2)\n","    aligned_blocks.sort(key=lambda item: item[0])\n","    if len(aligned_blocks) >= 3:\n","        return aligned_blocks[2][1].get('text', '').strip()\n","\n","    return None\n","\n","def extract_nom_de_pere_mere_idb(blocks, cin_text):\n","    pere_keywords = [\"FILS DE\", \"FILSDE\", \"FILEDE\", \"FILE DE\"]\n","    mere_keywords = [\"ET DE\", \"ETDE\"]\n","\n","    def starts_with_keyword(text, keywords):\n","        text_upper = text.upper().strip()\n","        return any(text_upper.startswith(k) for k in keywords)\n","\n","    # Step 1: Find CIN block\n","    cin_text_upper = cin_text.upper()\n","    cin_block = next((b for b in blocks if cin_text_upper in b.get(\"text\", \"\").upper()), None)\n","\n","    cin_bbox = cin_block[\"bbox\"]\n","    x_center = (cin_bbox[0] + cin_bbox[2]) / 2\n","    y_start = cin_bbox[3]\n","\n","    # Step 2: Find blocks with high confidence and matching keywords\n","    pere_block = None\n","    mere_block = None\n","    for block in blocks:\n","        if block.get(\"ocr_confidence\", 0) < 0.75:\n","            continue\n","        text = block.get(\"text\", \"\").strip()\n","        if starts_with_keyword(text, pere_keywords) and not pere_block:\n","            pere_block = block\n","        elif starts_with_keyword(text, mere_keywords) and not mere_block:\n","            mere_block = block\n","\n","    if pere_block and mere_block:\n","        return pere_block[\"text\"], mere_block[\"text\"]\n","\n","\n","    if not cin_block or \"bbox\" not in cin_block:\n","        return None, None\n","    # Step 3: Fallback using vertical line\n","    blocks_below = [\n","        b for b in blocks\n","        if b.get(\"bbox\") and b[\"bbox\"][1] > y_start\n","    ]\n","    blocks_below.sort(key=lambda b: b[\"bbox\"][1])\n","\n","    vertical_blocks = [\n","        b for b in blocks_below\n","        if b[\"bbox\"][0] <= x_center <= b[\"bbox\"][2]\n","    ]\n","\n","    selected_blocks = vertical_blocks[:2]\n","\n","    # Step 4: Use hybrid logic\n","    if pere_block or mere_block:\n","        if pere_block and not mere_block and len(selected_blocks) >= 2:\n","            return pere_block[\"text\"], selected_blocks[1][\"text\"]\n","        elif mere_block and not pere_block and len(selected_blocks) >= 2:\n","            return selected_blocks[0][\"text\"], mere_block[\"text\"]\n","        elif pere_block and not mere_block and len(selected_blocks) == 1:\n","            return pere_block[\"text\"], None\n","        elif mere_block and not pere_block and len(selected_blocks) == 1:\n","            return None, mere_block[\"text\"]\n","\n","    # Step 5: If no keyword blocks at all, use both from vertical line\n","    if len(selected_blocks) >= 2:\n","        return selected_blocks[0][\"text\"], selected_blocks[1][\"text\"]\n","    else:\n","        return None, None\n","\n","\n","def extract_idback_info(blocks):\n","\n","    cin_result, code_result, num_etat_civil_results, gender = extract_4_info_idb(blocks)\n","    extract_address_from_blocks_idb(blocks)\n","    try:\n","      nom_de_pere, nom_de_mere = extract_nom_de_pere_mere_idb(blocks, cin_result[0])\n","    except :\n","      nom_de_pere, nom_de_mere = None, None\n","\n","    result = {\n","        \"pays\": \"Maroc\",\n","        \"type_de_carte\": \"Carte Nationale d'Identité\",\n","        \"CIN\": (cin_result[0] if cin_result else None),\n","        \"Code\": (code_result[0] if code_result else None),\n","        \"Num_Civil\": (num_etat_civil_results[0] if num_etat_civil_results else None),\n","        \"Sexe\": gender,\n","        \"Pere\": nom_de_pere,\n","        \"Mere\": nom_de_mere\n","    }\n","\n","    return result\n"],"metadata":{"id":"QbuXCZe3yh43"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Defining Mean Functions for Extracting Information from the Front Side of Driver’s License"],"metadata":{"id":"eQ0az4YL4kPz"}},{"cell_type":"code","source":["def extract_prenom_pc(blocks, filtered_texts):\n","    def normalize(text):\n","        text = unicodedata.normalize(\"NFD\", text.lower().strip())\n","        return ''.join(c for c in text if unicodedata.category(c) != 'Mn')\n","\n","    def starts_with_label(text, label):\n","        return normalize(text).startswith(normalize(label))\n","\n","    def is_in_filtered(text):\n","        return any(text.strip() == ft['text'].strip() for ft in filtered_texts)\n","\n","    def find_label(labels, position_check=None):\n","        for i, block in enumerate(blocks):\n","            for label in labels:\n","                if starts_with_label(block['text'], label):\n","                    if not position_check or position_check(block['bbox']):\n","                        return block\n","        return None\n","\n","    def get_block_above(ref_block):\n","        ref_y = ref_block['bbox'][1]\n","        candidates = [\n","            block for block in blocks\n","            if block['bbox'][3] < ref_y and block['bbox'][0] < 0.5\n","        ]\n","        return sorted(candidates, key=lambda b: -b['bbox'][3])[0] if candidates else None\n","\n","    def get_blocks_below(ref_block, count):\n","        ref_y = ref_block['bbox'][3]\n","        candidates = [\n","            block for block in blocks\n","            if block['bbox'][1] > ref_y and block['bbox'][0] < 0.5\n","        ]\n","        return sorted(candidates, key=lambda b: b['bbox'][1])[:count]\n","\n","    # Step 1: Try \"Nom\"\n","    for label_group in [['Nom/', 'Nom', 'Nom...'], ['Prénom/', 'Prénom', 'Prénom...']]:\n","        label_type = 'Nom' if 'Nom' in label_group[0] else 'Prénom'\n","        label_block = find_label(label_group)\n","        if label_block:\n","            above = get_block_above(label_block)\n","            if above and is_in_filtered(above['text']):\n","                return above['text']\n","\n","            # Check below if Prénom case\n","            if label_type == 'Prénom':\n","                below = get_blocks_below(label_block, count=2)\n","                if below:\n","                    best = max(below, key=lambda b: b['ocr_confidence'])\n","                    if is_in_filtered(best['text']):\n","                        return best['text']\n","\n","    # Step 2: Fallback to CONDUIRE header\n","    def is_top_left(bbox):\n","        x, y = bbox[0], bbox[1]\n","        return x < 0.5 and y < 0.3\n","\n","    conduire_block = find_label(['PERMIS DE CONDUIRE'], position_check=is_top_left)\n","    if conduire_block:\n","        below = get_blocks_below(conduire_block, count=4)\n","        for b in below:\n","            if b['ocr_confidence'] > 0.87 and is_in_filtered(b['text']):\n","                return b['text']\n","\n","    return None\n","\n","def extract_nom_from_blocks_pc(blocks):\n","    keywords_sets = [\n","        {\"Date\", \"Naissance\", \"Lieu\"},\n","        {\"Date\", \"Naissance\"},\n","        {\"Naissance\"}\n","    ]\n","\n","    x_mins = [block['bbox'][0] for block in blocks]\n","    x_maxs = [block['bbox'][2] for block in blocks]\n","    card_center_x = (min(x_mins) + max(x_maxs)) / 2\n","\n","    blocks_sorted = sorted(blocks, key=lambda b: b['bbox'][1])\n","\n","    def contains_keywords(text, keywords):\n","        text_lower = text.lower()\n","        return all(k.lower() in text_lower for k in keywords)\n","\n","    def is_date(text):\n","        date_regex = re.compile(r'\\b\\d{2}[\\./-]\\d{2}[\\./-]\\d{4}\\b')\n","        return bool(date_regex.search(text))\n","\n","    for keywords in keywords_sets:\n","        for i, block in enumerate(blocks_sorted):\n","            if contains_keywords(block['text'], keywords):\n","                keyword_ymin = block['bbox'][1]\n","                candidate_blocks = [\n","                    b for b in blocks_sorted\n","                    if ((b['bbox'][0] + b['bbox'][2]) / 2) < card_center_x and b['bbox'][1] < keyword_ymin\n","                ]\n","                if candidate_blocks:\n","                    candidate_blocks = sorted(candidate_blocks, key=lambda b: b['bbox'][1], reverse=True)\n","                    return candidate_blocks[0]['text']\n","                else:\n","                    return blocks_sorted[i-1]['text'] if i > 0 else None\n","\n","    first_date_block = None\n","    for block in blocks_sorted:\n","        if is_date(block['text']):\n","            first_date_block = block\n","            break\n","\n","    if first_date_block:\n","        date_ymin = first_date_block['bbox'][1]\n","        above_blocks = [\n","            b for b in blocks_sorted\n","            if ((b['bbox'][0] + b['bbox'][2]) / 2) < card_center_x and b['bbox'][1] < date_ymin\n","        ]\n","        if above_blocks:\n","            above_blocks = sorted(above_blocks, key=lambda b: b['bbox'][1], reverse=True)\n","            block_above_date = above_blocks[0]\n","            block_above_date_ymin = block_above_date['bbox'][1]\n","            above_above_blocks = [\n","                b for b in blocks_sorted\n","                if ((b['bbox'][0] + b['bbox'][2]) / 2) < card_center_x and b['bbox'][1] < block_above_date_ymin\n","            ]\n","            if above_above_blocks:\n","                above_above_blocks = sorted(above_above_blocks, key=lambda b: b['bbox'][1], reverse=True)\n","                return above_above_blocks[0]['text']\n","\n","    return None\n","\n","def normalize_text_pc(text):\n","    text = text.lower()\n","    text = ''.join(\n","        c for c in unicodedata.normalize('NFD', text)\n","        if unicodedata.category(c) != 'Mn'\n","    )\n","    text = text.translate(str.maketrans('', '', string.punctuation))\n","    text = ' '.join(text.split())\n","    return text\n","\n","def extract_lieu_de_naissance_pc(boxes):\n","    variants = [\n","        \"délivré à\", \"delivre a\", \"delivré a\", \"délivre a\", \"dellivre a\",\n","        \"delivreà\", \"delivrea\", \"delivréà\", \"délivréà\",\n","        \"delivré a:\", \"délivré à:\", \"délivré à .\", \"délivré à :\",\n","    ]\n","\n","    date_pattern = re.compile(r'\\b\\d{2}/\\d{2}/\\d{4}\\b')\n","    date_boxes = [b for b in boxes if date_pattern.search(b['text'])]\n","\n","    if len(date_boxes) < 2:\n","        return None\n","\n","    date_boxes = sorted(date_boxes, key=lambda b: b['bbox'][1])\n","    first_date = date_boxes[0]\n","    second_date = date_boxes[1]\n","\n","    y_first_bottom = first_date['bbox'][3]\n","    y_second_top = second_date['bbox'][1]\n","    x_second_left = second_date['bbox'][0]\n","\n","    between_boxes = []\n","    for b in boxes:\n","        y_top, y_bottom = b['bbox'][1], b['bbox'][3]\n","        x_left = b['bbox'][0]\n","        text = b['text'].strip()\n","        conf = b['ocr_confidence']\n","\n","        if y_bottom <= y_first_bottom or y_top >= y_second_top:\n","            continue\n","\n","        if x_left >= x_second_left:\n","            continue\n","\n","        if b == second_date:\n","            continue\n","\n","        if y_bottom <= y_second_top and y_bottom > y_second_top - 0.05:\n","            text_norm = normalize_text_pc(text)\n","            if any(normalize_text_pc(v) in text_norm for v in variants):\n","                continue\n","\n","        if len(text) <= 2:\n","            continue\n","\n","        text_norm = normalize_text_pc(text)\n","        if any(normalize_text_pc(v) in text_norm for v in variants):\n","            continue\n","\n","        between_boxes.append(b)\n","\n","    high_conf_boxes = [b for b in between_boxes if b['ocr_confidence'] > 0.86]\n","\n","    if len(high_conf_boxes) >= 2:\n","        high_conf_boxes = [b for b in high_conf_boxes if b.get('classification', '').lower() != 'arabic']\n","\n","    if len(high_conf_boxes) == 0:\n","        candidate_boxes = between_boxes\n","    elif len(high_conf_boxes) == 1:\n","        candidate_boxes = high_conf_boxes\n","    else:\n","        candidate_boxes = high_conf_boxes\n","\n","    if len(candidate_boxes) == 1:\n","        selected_text = candidate_boxes[0]['text']\n","    elif len(candidate_boxes) > 1:\n","        candidate_boxes = sorted(candidate_boxes, key=lambda b: b['ocr_confidence'], reverse=True)\n","        selected_text = candidate_boxes[0]['text']\n","    else:\n","        selected_text = None\n","\n","    return selected_text\n","\n","def remove_accents_pc(text):\n","    return ''.join(\n","        c for c in unicodedata.normalize('NFD', text)\n","        if unicodedata.category(c) != 'Mn'\n","    )\n","\n","def extract_lieu_delivrance_pc(blocks):\n","    variants = [\n","        \"délivré à\", \"delivre a\", \"delivré a\", \"délivre a\", \"dellivre a\",\n","        \"delivreà\", \"delivrea\", \"delivréà\", \"délivréà\",\n","        \"delivré a:\", \"délivré à:\", \"délivré à .\", \"délivré à :\",\n","    ]\n","\n","    # 1. First try exact match ignoring case (with accents)\n","    for block in blocks:\n","        original_text = block.get('text', '').strip()\n","        lower_text = original_text.lower()\n","        for variant in variants:\n","            if variant.lower() in lower_text:\n","                idx = lower_text.find(variant.lower())\n","                length = len(variant)\n","                after = original_text[idx + length:].strip(\" :.\")\n","                if after:\n","                    return after\n","\n","    # 2. If no match found, try ignoring accents\n","    variants_no_accents = [remove_accents_pc(v).lower() for v in variants]\n","\n","    for block in blocks:\n","        original_text = block.get('text', '').strip()\n","        text_no_accents = remove_accents_pc(original_text).lower()\n","\n","        for variant_no_accents in variants_no_accents:\n","            if variant_no_accents in text_no_accents:\n","                idx = text_no_accents.find(variant_no_accents)\n","                length = len(variant_no_accents)\n","                after = original_text[idx + length:].strip(\" :.\")\n","                if after:\n","                    return after\n","\n","    return None\n","\n","def extract_license_number_pc(blocks):\n","    pattern = re.compile(r'\\b\\d{2}/\\d{6}\\b')\n","\n","    for block in blocks:\n","        text = block.get('text', '')\n","        match = pattern.search(text)\n","        if match:\n","            return match.group(0)\n","\n","    return None\n","\n","def extract_cin_from_blocks_pc(blocks):\n","    pattern = re.compile(r'\\b[A-Z]{1,3}\\d{4,10}\\b')\n","    results = []\n","    for block in blocks:\n","        text = block.get('text', '').upper()\n","        matches = pattern.findall(text)\n","        results.extend(matches)\n","    return results\n","\n","def extract_dates_and_categories_pc(blocks):\n","    date_pattern = re.compile(r'\\b(\\d{2})[\\s./-](\\d{2})[\\s./-](\\d{4})\\b')\n","    categories_list = {\"AM\", \"A1\", \"A\", \"B\", \"C\", \"D\", \"EB\", \"EC\", \"ED\"}\n","\n","    result = {\n","        \"date_de_naissance\": None,\n","        \"Delivre_le\": None,\n","        \"categories\": []\n","    }\n","\n","    # Filter blocks with decent OCR confidence\n","    filtered_blocks = [\n","        (block['text'], block['bbox'][1])\n","        for block in blocks\n","        if block.get('ocr_confidence', 0) > 0.7\n","    ]\n","\n","    # Extract dates with y-position\n","    dates_with_positions = []\n","    for text, y in filtered_blocks:\n","        for match in date_pattern.finditer(text):\n","            dates_with_positions.append((match.group(0), y))\n","    dates_with_positions.sort(key=lambda x: x[1])  # sort top to bottom\n","\n","    # Assign dates\n","    if len(dates_with_positions) >= 2:\n","        result[\"date_de_naissance\"] = dates_with_positions[0][0]\n","        result[\"Delivre_le\"] = dates_with_positions[1][0]\n","        delivre_le_y = dates_with_positions[1][1]\n","    elif len(dates_with_positions) == 1:\n","        result[\"Delivre_le\"] = dates_with_positions[0][0]\n","        delivre_le_y = dates_with_positions[0][1]\n","    else:\n","        delivre_le_y = None\n","\n","    # Extract categories from left-side blocks only (after Delivre_le)\n","    if delivre_le_y is not None:\n","        margin = 0.01\n","        left_below_blocks = [\n","            block for block in blocks\n","            if (\n","                block['bbox'][1] > delivre_le_y + margin and\n","                block.get('ocr_confidence', 0) > 0.6 and\n","                ((block['bbox'][0] + block['bbox'][2]) / 2) < 0.5  # left side\n","            )\n","        ]\n","\n","        for block in left_below_blocks:\n","            words = re.findall(r'\\b\\w+\\b', block['text'].upper())\n","            for w in words:\n","                if w in categories_list and w not in result[\"categories\"]:\n","                    result[\"categories\"].append(w)\n","\n","    return result\n","\n","def extract_permis_info(filtered_texts, ocr_results):\n","    result = {\n","        \"pays\": \"Maroc\",\n","        \"type_de_carte\": \"PERMIS DE CONDUIRE\",\n","        \"prenom\": None,\n","        \"nom\": None,\n","        \"lieu_naissance\": None,\n","        \"lieu_delivrance\": None,\n","        \"numero_permis\": None,\n","        \"date_de_naissance\": None,\n","        \"delivre_le\": None,\n","        \"categories\": [],\n","        \"CIN\": None\n","    }\n","\n","    # Extract fields from OCR blocks\n","    result[\"prenom\"] = extract_prenom_pc(ocr_results, filtered_texts)\n","    result[\"nom\"] = extract_nom_from_blocks_pc(ocr_results)\n","\n","    # Extract locations\n","    result[\"lieu_naissance\"] = extract_lieu_de_naissance_pc(filtered_texts)\n","    result[\"lieu_delivrance\"] = extract_lieu_delivrance_pc(filtered_texts)\n","\n","    # Extract license number\n","    result[\"numero_permis\"] = extract_license_number_pc(filtered_texts)\n","\n","    # Extract dates and categories\n","    date_info = extract_dates_and_categories_pc(filtered_texts)\n","    result[\"date_de_naissance\"] = date_info.get(\"date_de_naissance\")\n","    result[\"delivre_le\"] = date_info.get(\"Delivre_le\")\n","    result[\"categories\"] = date_info.get(\"categories\", [])\n","\n","    # Extract CIN\n","    CIN = extract_cin_from_blocks_pc(filtered_texts)\n","    result[\"CIN\"] = CIN\n","\n","    return result"],"metadata":{"id":"kF2Y6qn_dohG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Defining Mean Functions for Extracting Information from PASSPORT"],"metadata":{"id":"BnWbydlW4u7h"}},{"cell_type":"code","source":["def extract_passport_dates(blocks):\n","    date_pattern = re.compile(r'\\b(\\d{2})[/-](\\d{2})[/-](\\d{4})\\b')\n","\n","    date_blocks = []\n","\n","    for block in blocks:\n","        if block.get(\"ocr_confidence\", 0) < 0.4:\n","            continue\n","\n","        text = block[\"text\"]\n","        for match in date_pattern.finditer(text):\n","            date_str = match.group(0)\n","            day, month, year = match.groups()\n","            y_position = block[\"bbox\"][1]\n","            date_blocks.append({\n","                \"text\": date_str,\n","                \"year\": int(year),\n","                \"y\": y_position\n","            })\n","\n","    # Sort top-to-bottom by y-position\n","    date_blocks.sort(key=lambda x: x[\"y\"])\n","\n","    result = {\n","        \"date_de_naissance\": None,\n","        \"date_de_delivrance\": None,\n","        \"date_dexpiration\": None\n","    }\n","\n","    # Try to find the first valid trio in chronological order\n","    for i in range(len(date_blocks) - 2):\n","        d1, d2, d3 = date_blocks[i], date_blocks[i+1], date_blocks[i+2]\n","        if d1[\"year\"] < d2[\"year\"] < d3[\"year\"]:\n","            result[\"date_de_naissance\"] = d1[\"text\"]\n","            result[\"date_de_delivrance\"] = d2[\"text\"]\n","            result[\"date_dexpiration\"] = d3[\"text\"]\n","            return result\n","\n","    return result\n","\n","\n","def extract_authorite(blocks, date_delivrance_text, y_tolerance=0.02):\n","    date_block = None\n","    for block in blocks:\n","        if block.get(\"ocr_confidence\", 0) < 0.6:\n","            continue\n","        if date_delivrance_text in block[\"text\"]:\n","            date_block = block\n","            break\n","    if not date_block:\n","        return None\n","    date_y = date_block[\"bbox\"][1]\n","    date_x_max = date_block[\"bbox\"][2]\n","\n","    candidates = []\n","    for block in blocks:\n","        if block.get(\"ocr_confidence\", 0) < 0.6:\n","            continue\n","        bbox = block[\"bbox\"]\n","        block_y_top = bbox[1]\n","        block_y_bottom = bbox[3]\n","        if abs(block_y_top - date_y) < y_tolerance or abs(block_y_bottom - date_y) < y_tolerance:\n","            if bbox[0] > date_x_max:\n","                candidates.append(block)\n","\n","    if not candidates:\n","        return None\n","    candidates.sort(key=lambda b: b[\"bbox\"][0])\n","    return candidates[0][\"text\"]\n","\n","\n","def extract_passport_sexe(blocks, birth_date):\n","    if not birth_date:\n","        return None\n","\n","    target_block = None\n","    margin_y = 0.01  # Vertical tolerance for alignment\n","\n","    # Step 1: Find the block containing the birth date\n","    for block in blocks:\n","        if birth_date in block.get(\"text\", \"\"):\n","            target_block = block\n","            break\n","\n","    if not target_block:\n","        return None\n","\n","    birth_y_min = target_block[\"bbox\"][1]\n","    birth_y_max = target_block[\"bbox\"][3]\n","    birth_x_min = target_block[\"bbox\"][0]\n","\n","    # Step 2: Scan all blocks to the left on the same line\n","    candidates = []\n","    for block in blocks:\n","        text = block.get(\"text\", \"\").strip().upper()\n","        x_min, y_min, x_max, y_max = block[\"bbox\"]\n","\n","        # Must be aligned with the birth date row\n","        if y_min >= birth_y_min - margin_y and y_max <= birth_y_max + margin_y:\n","            # Must be to the left of birth date block\n","            if x_max <= birth_x_min:\n","                # Allow max 3 characters with M or F inside\n","                if len(text) <= 3 and any(letter in text for letter in (\"M\", \"F\")):\n","                    candidates.append((x_min, text))\n","\n","    # Step 3: Return the closest valid gender marker to the birth date\n","    if candidates:\n","        candidates.sort(reverse=True)  # rightmost valid candidate first\n","        for _, text in candidates:\n","            if \"M\" in text:\n","                return \"M\"\n","            if \"F\" in text:\n","                return \"F\"\n","\n","    return None\n","\n","def extract_cin_and_passport_number(blocks):\n","    passport_regex = re.compile(r'\\b[A-Z]{2}\\d{7}\\b')\n","    cin_regex = re.compile(r'\\b[A-Z]{1,2}\\d{4,7}\\b')    #\n","\n","    passport_candidates = []\n","    cin_candidates = []\n","\n","    for block in blocks:\n","        text = block.get(\"text\", \"\").strip().upper()\n","        if not text:\n","            continue\n","\n","        x_min, y_min, x_max, y_max = block[\"bbox\"]\n","\n","        # Check for passport number (top right region)\n","        if passport_regex.fullmatch(text) and x_min >= 0.5 and y_min <= 0.4:\n","            passport_candidates.append((y_min, block))\n","\n","        # Check for CIN (bottom left region)\n","        if cin_regex.fullmatch(text) and x_max <= 0.5 and y_max >= 0.6:\n","            cin_candidates.append((y_max, block))\n","\n","    result = {\n","        \"numero_passport\": passport_candidates[0][1][\"text\"] if passport_candidates else None,\n","        \"cin\": cin_candidates[0][1][\"text\"] if cin_candidates else None\n","    }\n","\n","    return result\n","\n","\n","def extract_passport_type(blocks, passport_number):\n","    passport_types = {\"P\", \"PP\", \"PD\", \"PS\", \"S\", \"PL\"}\n","\n","    passport_number = passport_number.upper()\n","    target_block = None\n","\n","    for block in blocks:\n","        text = block.get(\"text\", \"\").strip().upper()\n","        if text == passport_number:\n","            target_block = block\n","            break\n","\n","    if not target_block:\n","        return None\n","\n","    y_center = (target_block[\"bbox\"][1] + target_block[\"bbox\"][3]) / 2\n","    x_min = target_block[\"bbox\"][0]\n","\n","    candidates = []\n","\n","    for block in blocks:\n","        if block == target_block:\n","            continue\n","        y_block_center = (block[\"bbox\"][1] + block[\"bbox\"][3]) / 2\n","        x_max = block[\"bbox\"][2]\n","\n","        if abs(y_block_center - y_center) < 0.02:\n","            if x_max < x_min:\n","                text = block.get(\"text\", \"\").strip().upper()\n","                if text in passport_types:\n","                    candidates.append((x_max, text))\n","\n","    if candidates:\n","        candidates.sort(key=lambda x: x[0], reverse=True)\n","        return candidates[0][1]\n","\n","    # Fallback: check last two lines of blocks for a starting string matching passport types\n","    blocks_sorted = sorted(blocks, key=lambda b: b[\"bbox\"][1], reverse=True)\n","    last_two_lines = blocks_sorted[:2]\n","\n","    for block in last_two_lines:\n","        text = block.get(\"text\", \"\").strip().upper()\n","        if not text:\n","            continue\n","        prefix = text[:2]\n","        if prefix[0] == \"P\" and prefix not in passport_types:\n","            return \"P\"\n","        if prefix in passport_types:\n","            return prefix\n","\n","    return None\n","\n","import unicodedata\n","\n","def normalize_text(text):\n","    return unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('ASCII').lower()\n","\n","def extract_nom_passport(blocks, numero_passport=None):\n","    def find_blocks_containing(substring, accent_sensitive=True):\n","        results = []\n","        search_str = substring.lower() if accent_sensitive else normalize_text(substring)\n","        for block in blocks:\n","            text = block.get(\"text\", \"\")\n","            target_text = text if accent_sensitive else normalize_text(text)\n","            if search_str in target_text:\n","                results.append(block)\n","        return results\n","\n","    def left_side_blocks_below(target_block, count=2):\n","        y_max = target_block[\"bbox\"][3]\n","        candidates = []\n","        for block in blocks:\n","            bx_min, by_min, bx_max, by_max = block[\"bbox\"]\n","            if by_min > y_max and bx_max <= 0.5:\n","                candidates.append(block)\n","        candidates.sort(key=lambda b: b[\"bbox\"][1])\n","        return candidates[:count]\n","\n","    prenoms_blocks = [b for b in blocks if b.get(\"text\",\"\").lower().startswith(\"prénoms\")]\n","    if prenoms_blocks:\n","        block = prenoms_blocks[0]\n","        above_candidates = [b for b in blocks if abs(b[\"bbox\"][1] - block[\"bbox\"][1]) < 0.05 and b[\"bbox\"][3] < block[\"bbox\"][1] and b[\"bbox\"][0] < 0.5]\n","        if above_candidates:\n","            above_candidates.sort(key=lambda b: b[\"bbox\"][1], reverse=True)\n","            return above_candidates[0][\"text\"]\n","\n","    prenoms_blocks = find_blocks_containing(\"prenoms\", accent_sensitive=False)\n","    if prenoms_blocks:\n","        block = prenoms_blocks[0]\n","        above_candidates = [b for b in blocks if abs(b[\"bbox\"][1] - block[\"bbox\"][1]) < 0.05 and b[\"bbox\"][3] < block[\"bbox\"][1] and b[\"bbox\"][0] < 0.5]\n","        if above_candidates:\n","            above_candidates.sort(key=lambda b: b[\"bbox\"][1], reverse=True)\n","            return above_candidates[0][\"text\"]\n","\n","    given_blocks = []\n","    for b in blocks:\n","        text_norm = normalize_text(b.get(\"text\",\"\"))\n","        if \"given\" in text_norm:\n","            given_blocks.append(b)\n","    if given_blocks:\n","        block = given_blocks[0]\n","        above_candidates = [b for b in blocks if abs(b[\"bbox\"][1] - block[\"bbox\"][1]) < 0.05 and b[\"bbox\"][3] < block[\"bbox\"][1] and b[\"bbox\"][0] < 0.5]\n","        if above_candidates:\n","            above_candidates.sort(key=lambda b: b[\"bbox\"][1], reverse=True)\n","            return above_candidates[0][\"text\"]\n","\n","    nom_blocks = []\n","    for b in blocks:\n","        text = b.get(\"text\",\"\").lower()\n","        if text.startswith(\"nom\") or text.startswith(\"nom/\") or text.startswith(\"noml\") or text.startswith(\"nom/name\"):\n","            nom_blocks.append(b)\n","    if nom_blocks:\n","        block = nom_blocks[0]\n","        below_candidates = left_side_blocks_below(block, count=2)\n","        if not below_candidates:\n","            return None\n","        if len(below_candidates) == 1:\n","            return below_candidates[0][\"text\"]\n","        c1, c2 = below_candidates[0], below_candidates[1]\n","        c1_conf = c1.get(\"ocr_confidence\",0)\n","        c2_conf = c2.get(\"ocr_confidence\",0)\n","        if c1_conf > 0.86 and c2_conf > 0.86:\n","            c1_class = c1.get(\"classification\",\"\").lower()\n","            c2_class = c2.get(\"classification\",\"\").lower()\n","            if c1_class == \"french\" and c2_class == \"french\":\n","                return c2[\"text\"]\n","            elif c1_class == \"french\":\n","                return c1[\"text\"]\n","            elif c2_class == \"french\":\n","                return c2[\"text\"]\n","            else:\n","                return c1[\"text\"]\n","        elif c1_conf > 0.86:\n","            return c1[\"text\"]\n","        elif c2_conf > 0.86:\n","            return c2[\"text\"]\n","        else:\n","            return c1[\"text\"]\n","\n","    if numero_passport:\n","        target_block = None\n","        for b in blocks:\n","            if b.get(\"text\",\"\").strip().upper() == numero_passport.upper():\n","                target_block = b\n","                break\n","        if target_block:\n","            y_max = target_block[\"bbox\"][3]\n","            candidates = []\n","            for b in blocks:\n","                bx_min, by_min, bx_max, by_max = b[\"bbox\"]\n","                if by_min > y_max and bx_max <= 0.5:\n","                    candidates.append(b)\n","            candidates.sort(key=lambda b: b[\"bbox\"][1])\n","            candidates = candidates[:3]\n","            if not candidates:\n","                return None\n","            if len(candidates) == 1:\n","                return candidates[0][\"text\"]\n","            filtered = [c for c in candidates if c.get(\"ocr_confidence\",0) > 0.86]\n","            if filtered:\n","                french_filtered = [c for c in filtered if c.get(\"classification\",\"\").lower() == \"french\"]\n","                if french_filtered:\n","                    return french_filtered[0][\"text\"]\n","                return filtered[0][\"text\"]\n","            return candidates[0][\"text\"]\n","\n","    return None\n","\n","def extract_prenom_passport(ocr_blocks, nom_value=None):\n","\n","    # Helper function to find text with tolerance\n","    def find_text_pattern(pattern, blocks):\n","        results = []\n","        for block in blocks:\n","            text = block.get('text', '').upper()\n","            if re.search(pattern, text):\n","                results.append(block)\n","        return results\n","\n","    # Improved spatial relationship function\n","    def get_boxes_in_relation(target_block, blocks, relation='above', x_tolerance=0.15, max_distance=0.3):\n","        target_bbox = target_block.get('bbox', (0, 0, 0, 0))\n","        target_center_x = (target_bbox[0] + target_bbox[2]) / 2\n","\n","        candidates = []\n","        for block in blocks:\n","            if block == target_block:\n","                continue\n","\n","            block_bbox = block.get('bbox', (0, 0, 0, 0))\n","            block_center_x = (block_bbox[0] + block_bbox[2]) / 2\n","\n","            # Check if in same vertical column\n","            if abs(block_center_x - target_center_x) < x_tolerance:\n","                distance = None\n","\n","                if relation == 'above' and block_bbox[3] < target_bbox[1]:\n","                    distance = target_bbox[1] - block_bbox[3]\n","                elif relation == 'below' and block_bbox[1] > target_bbox[3]:\n","                    distance = block_bbox[1] - target_bbox[3]\n","\n","                if distance is not None and distance < max_distance:\n","                    candidates.append((block, distance))\n","\n","        # Sort by distance and return\n","        return [block for block, dist in sorted(candidates, key=lambda x: x[1])]\n","\n","    # TRY 1: Look for explicit \"Prénoms\" or \"Given Names\" label (MOST RELIABLE)\n","    prenoms_blocks = find_text_pattern(r'PRÉNOMS|PRENOMS|GIVEN.*NAMES', ocr_blocks)\n","\n","    if prenoms_blocks:\n","        prenoms_block = prenoms_blocks[0]\n","        boxes_below = get_boxes_in_relation(prenoms_block, ocr_blocks, 'below')\n","\n","        # Look for the actual given name below the label\n","        for box in boxes_below:\n","            text = box.get('text', '').strip()\n","            # More strict validation for given names\n","            if (2 <= len(text) <= 20 and\n","                re.match(r'^[A-Z][A-Za-z\\.]*$', text) and\n","                not any(keyword in text.upper() for keyword in ['MAROC', 'MAROCAIN', 'PASSEPORT', 'NATIONAL', 'SEXE', 'DATE']) and\n","                not re.search(r'\\d', text)):\n","                return text\n","\n","    # TRY 2: Use nom value with BETTER filtering\n","    if nom_value:\n","        # Find the nom block\n","        nom_blocks = []\n","        clean_nom = re.sub(r'[^A-Z]', '', nom_value.upper())\n","\n","        for block in ocr_blocks:\n","            clean_text = re.sub(r'[^A-Z]', '', block.get('text', '').upper())\n","            if clean_nom in clean_text and len(clean_text) >= len(clean_nom) * 0.8:\n","                nom_blocks.append(block)\n","\n","        if nom_blocks:\n","            nom_block = nom_blocks[0]\n","            boxes_below = get_boxes_in_relation(nom_block, ocr_blocks, 'below')\n","\n","            # Check if any box below looks like a given name with BETTER filtering\n","            for box in boxes_below:\n","                text = box.get('text', '').strip()\n","\n","                # Skip nationality words and other non-name text\n","                if any(keyword in text.upper() for keyword in ['MAROCAIN', 'MAROCAINE', 'NATIONAL', 'SEXE', 'DATE']):\n","                    continue\n","\n","                # Skip text with numbers or special characters (except dots for initials)\n","                if re.search(r'[^A-Za-z\\.]', text) and not text.endswith('.'):\n","                    continue\n","\n","                # Look for actual name text with reasonable length\n","                if (2 <= len(text) <= 20 and re.match(r'^[A-Z][A-Za-z\\.]*$', text)):\n","                    return text\n","\n","    # TRY 3: Find blocks containing \"Nationalit\" pattern\n","    nationalite_blocks = find_text_pattern(r'NATIONA(LIT|LITY|TÉ|TY)', ocr_blocks)\n","    if nationalite_blocks:\n","        nationalite_block = nationalite_blocks[0]\n","        boxes_above = get_boxes_in_relation(nationalite_block, ocr_blocks, 'above')\n","\n","        # Look for boxes that are NOT the surname and look like given names\n","        for box in boxes_above:\n","            text = box.get('text', '').strip()\n","\n","            # Skip if this is likely the surname (matches provided nom_value)\n","            if nom_value and nom_value.upper() in text.upper():\n","                continue\n","\n","            # Skip label boxes and nationality words\n","            if any(keyword in text.upper() for keyword in ['PRÉNOMS', 'PRENOMS', 'GIVEN', 'NOM', 'NAME', 'MAROCAIN']):\n","                continue\n","\n","            # Skip non-name text\n","            if re.match(r'^[^A-Za-z]*$', text) or re.search(r'\\d', text):\n","                continue\n","\n","            # This should be the given name!\n","            if 2 <= len(text) <= 20:\n","                return text\n","\n","    # TRY 4: Direct search for name patterns (fallback)\n","    for block in ocr_blocks:\n","        text = block.get('text', '').strip()\n","        # Look for text that matches name patterns but excludes known non-name words\n","        if (2 <= len(text) <= 20 and\n","            re.match(r'^[A-Z][A-Za-z\\.]*$', text) and\n","            not any(keyword in text.upper() for keyword in ['MAROC', 'PASSEPORT', 'NOM', 'PRÉNOMS', 'NATIONAL', 'SEXE', 'DATE', 'CARD']) and\n","            (not nom_value or nom_value.upper() not in text.upper())):\n","            return text\n","\n","    return None\n","\n","def get_vertical_strip_between_dates(ocr_data, date1, date2, overlap_margin=0.001):\n","\n","    def find_bbox_by_text(target_text):\n","        for entry in ocr_data:\n","            if target_text.strip() in entry['text']:\n","                return entry['bbox']\n","        return None\n","\n","    bbox1 = find_bbox_by_text(date1)\n","    bbox2 = find_bbox_by_text(date2)\n","\n","    if not bbox1 or not bbox2:\n","        raise ValueError(f\"One or both dates not found: '{date1}', '{date2}'\")\n","\n","    y1_top, y1_bottom = bbox1[1], bbox1[3]\n","    y2_top, y2_bottom = bbox2[1], bbox2[3]\n","\n","    # Define the vertical range between dates\n","    y_top = min(y1_bottom, y2_bottom)\n","    y_bottom = max(y1_top, y2_top)\n","\n","    def overlaps(a_top, a_bottom, b_top, b_bottom, margin=0.001):\n","        return not (a_bottom < b_top + margin or a_top > b_bottom - margin)\n","\n","    # Filter entries that are within vertical range and NOT overlapping date lines\n","    filtered = []\n","    for entry in ocr_data:\n","        e_top, e_bottom = entry['bbox'][1], entry['bbox'][3]\n","\n","        if y_top < e_top and e_bottom < y_bottom:\n","            # Fully between the dates\n","            filtered.append(entry)\n","        else:\n","            if overlaps(e_top, e_bottom, y1_top, y1_bottom, overlap_margin):\n","                continue\n","            if overlaps(e_top, e_bottom, y2_top, y2_bottom, overlap_margin):\n","                continue\n","            if y_top < e_top and e_bottom < y_bottom:\n","                filtered.append(entry)\n","\n","    return filtered\n","\n","def split_blocks_by_left_right(blocks):\n","\n","    # Calculate x-centers for all blocks\n","    x_centers = []\n","    for block in blocks:\n","        x_min, _, x_max, _ = block['bbox']\n","        x_center = (x_min + x_max) / 2\n","        x_centers.append(x_center)\n","\n","    # Compute the median x_center as the vertical split\n","    median_x = sorted(x_centers)[len(x_centers) // 2]\n","\n","    left_blocks = []\n","    right_blocks = []\n","\n","    for block, x_center in zip(blocks, x_centers):\n","        if x_center < median_x:\n","            left_blocks.append(block)\n","        else:\n","            right_blocks.append(block)\n","\n","    return left_blocks, right_blocks\n","\n","def extract_closest_field(texts):\n","    label_keywords = ['lieu', 'birth', 'domicile', 'date', 'naissanca', 'place', 'délivr']\n","    fallback_keyword = 'maroc'\n","\n","    def is_close_word(word, keywords, max_dist=2):\n","        \"\"\"Return True if word is within max_dist edit distance of any keyword\"\"\"\n","        return any(difflib.SequenceMatcher(None, word, kw).ratio() >= (1 - max_dist / max(len(word), len(kw))) for kw in keywords)\n","\n","    # Only one box\n","    if len(texts) == 1:\n","        entry = texts[0]\n","        if entry.get('classification', '').lower() == 'french' or entry.get('ocr_confidence', 0) > 0.5:\n","            return [entry['text']]\n","        else:\n","            return None\n","\n","    matched_labels = []\n","\n","    # Try to find label using exact keyword match\n","    for entry in texts:\n","        text = entry['text'].lower()\n","        if any(kw in text for kw in label_keywords):\n","            matched_labels.append(entry)\n","\n","    results = []\n","\n","    # Use closest box to label(s)\n","    for label in matched_labels:\n","        lx_min, ly_min, lx_max, ly_max = label['bbox']\n","        label_center = ((lx_min + lx_max) / 2, (ly_min + ly_max) / 2)\n","\n","        closest_candidate = None\n","        closest_distance = float('inf')\n","\n","        for candidate in texts:\n","            if candidate == label:\n","                continue\n","\n","            cx_min, cy_min, cx_max, cy_max = candidate['bbox']\n","            candidate_center = ((cx_min + cx_max) / 2, (cy_min + cy_max) / 2)\n","\n","            distance = np.linalg.norm(np.array(candidate_center) - np.array(label_center))\n","\n","            if distance < closest_distance:\n","                closest_distance = distance\n","                closest_candidate = candidate\n","\n","        if closest_candidate:\n","            results.append(closest_candidate['text'])\n","\n","    # Fallback – look for \"maroc\" in any text\n","    if not results:\n","        for entry in texts:\n","            if fallback_keyword in entry['text'].lower():\n","                results.append(entry['text'])\n","                break\n","\n","    # Fuzzy label detection for 2-box edge case\n","    if not results and len(texts) == 2:\n","        for entry in texts:\n","            text_words = entry['text'].lower().split()\n","            for word in text_words:\n","                if is_close_word(word, label_keywords, max_dist=2):\n","                    # Return the *other* entry as the candidate\n","                    other = [e for e in texts if e != entry]\n","                    if other:\n","                        return [other[0]['text']]\n","\n","    return results if results else None\n","\n","\n","def looks_like_address(text):\n","    \"\"\"\n","    Return True if text contains symbols typically found in address blocks.\n","    \"\"\"\n","    symbols = set('@[{!#%*]')\n","    return any(char in text for char in symbols)\n","\n","def fuzzy_match_in_text(text, keywords, max_distance=2):\n","    \"\"\"\n","    Check if any keyword approximately appears inside the text allowing max_distance errors.\n","    \"\"\"\n","    text = text.lower()\n","    for keyword in keywords:\n","        keyword = keyword.lower()\n","        # Slide over the text with a window equal to keyword length and check fuzzy similarity\n","        length = len(keyword)\n","        for i in range(len(text) - length + 1):\n","            window = text[i:i+length]\n","            # Calculate Levenshtein-like distance using difflib SequenceMatcher ratio\n","            seq = difflib.SequenceMatcher(None, keyword, window)\n","            similarity = seq.ratio()\n","            # similarity of 1.0 means exact match; allow some fuzziness, e.g. >= 0.7\n","            if similarity >= 0.7:\n","                # Allow some differences in length by checking actual edit distance as well\n","                # But difflib ratio should be enough here\n","                return True\n","    return False\n","\n","def extract_address(blocks, authorite_text):\n","    label_keywords = ['domicile', 'residence', 'authorit', 'authority', 'lieu', 'authori']\n","\n","    # Locate and exclude the authorite block and anything below it\n","    authorite_box = None\n","    for block in blocks:\n","        if authorite_text and authorite_text.lower() in block['text'].lower():\n","            authorite_box = block\n","            break\n","\n","    if authorite_box:\n","        _, a_ymin, _, _ = authorite_box['bbox']\n","        blocks = [b for b in blocks if b['bbox'][1] < a_ymin]\n","\n","    if not blocks:\n","        return None\n","\n","    # If only one block, return it if it looks like a valid address\n","    if len(blocks) == 1:\n","        b = blocks[0]\n","        if (\n","            b['classification'].lower() == 'french'\n","            or b['ocr_confidence'] > 0.5\n","            or looks_like_address(b['text'])\n","        ):\n","            return b['text']\n","        return None\n","\n","    # Remove any label-like blocks using fuzzy matching on the full text\n","    filtered_blocks = []\n","    for b in blocks:\n","        if fuzzy_match_in_text(b['text'], label_keywords):\n","            # This block looks like a label, so skip it\n","            continue\n","        filtered_blocks.append(b)\n","\n","    if not filtered_blocks:\n","        return None\n","\n","    # Return cleaned result depending on how many blocks are left\n","    if len(filtered_blocks) == 1:\n","        return filtered_blocks[0]['text']\n","    elif len(filtered_blocks) == 2:\n","        return f\"{filtered_blocks[0]['text']} {filtered_blocks[1]['text']}\"\n","    else:\n","        # Sort left-to-right (x_min) if more than 2 blocks\n","        filtered_blocks.sort(key=lambda b: b['bbox'][0])\n","        return ' '.join(b['text'] for b in filtered_blocks)\n","\n","def extract_passport_info(filtered_texts):\n","    # Extract all pieces\n","    cin_num = extract_cin_and_passport_number(filtered_texts) or {}\n","    dates = extract_passport_dates(filtered_texts) or {}\n","    authorite = extract_authorite(filtered_texts, dates.get('date_de_delivrance', '')) or ''\n","    sexe = extract_passport_sexe(filtered_texts, dates.get('date_de_naissance', '')) or ''\n","    passport_type = extract_passport_type(filtered_texts, cin_num.get('numero_passport', '')) or ''\n","    nom = extract_nom_passport(filtered_texts, cin_num.get('numero_passport', '')) or ''\n","    prenom = extract_prenom_passport(filtered_texts, nom_value=nom) or ''\n","\n","    # Filter text vertically between dates, split into left/right blocks\n","    filt_text = get_vertical_strip_between_dates(filtered_texts, dates.get('date_de_naissance', ''), dates.get('date_de_delivrance', ''))\n","    left_blocks, right_blocks = split_blocks_by_left_right(filt_text)\n","\n","    place_of_birth_list = extract_closest_field(left_blocks) or []\n","    place_of_birth = place_of_birth_list[0] if place_of_birth_list else ''\n","\n","    address = extract_address(right_blocks, authorite) or ''\n","\n","    # Compose flat dictionary\n","    result = {\n","        \"pays\": \"Maroc\",\n","        \"type_de_carte\": \"PASSPORT\",\n","        \"Naionalite\": \"Marocaine\",\n","        'numero_passport': cin_num.get('numero_passport', ''),\n","        'cin': cin_num.get('cin', ''),\n","        'date_de_naissance': dates.get('date_de_naissance', ''),\n","        'date_de_delivrance': dates.get('date_de_delivrance', ''),\n","        'date_dexpiration': dates.get('date_dexpiration', ''),\n","        'authorite': authorite,\n","        'sexe': sexe,\n","        'passport_type': passport_type,\n","        'nom': nom,\n","        'prenom': prenom,\n","        'place_of_birth': place_of_birth,\n","        'address': address,\n","    }\n","\n","    return result\n"],"metadata":{"id":"02trL6ZkCuTv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# DOC Type Classification"],"metadata":{"id":"QQCF5aea48dQ"}},{"cell_type":"code","source":["def normalize(text):\n","    text = unicodedata.normalize(\"NFD\", text.lower().strip())\n","    return ''.join(c for c in text if unicodedata.category(c) != 'Mn')\n","\n","def classify_card_type(blocks):\n","\n","    all_texts = [normalize(block.get('text', '')) for block in blocks]\n","\n","    date_pattern = re.compile(r'\\b\\d{2}[\\s./-]\\d{2}[\\s./-]\\d{4}\\b')\n","    date_count = sum(len(date_pattern.findall(text)) for text in all_texts)\n","\n","    has_id_card = False\n","    has_permis = False\n","    has_passport = False\n","\n","    for text in all_texts:\n","        if text == normalize(\"CARTE NATIONALE D'IDENTITE\"):\n","            has_id_card = True\n","        elif \"carte nationale\" in text:\n","            has_id_card = True\n","        elif \"valable jusqu\" in text:\n","            has_id_card = True\n","        elif text == normalize(\"PERMIS DE CONDUIRE\"):\n","            has_permis = True\n","        elif \"conduire\" in text:\n","            has_permis = True\n","        elif \"passeport\" in text:\n","            has_passport = True\n","        elif \"kingdom of morocco\" in text or \"kingdom\" in text:\n","            has_passport = True\n","\n","    if has_passport:\n","        return \"passport\"\n","    if has_permis:\n","        return \"permis_de_conduire\"\n","    if date_count == 0:\n","        return \"id_card_back\"\n","    if date_count >= 3:\n","        return \"passport\"\n","    if has_id_card:\n","        return \"id_card_front\"\n","    if date_count <= 2:\n","        return \"id_card_front\"\n","    return \"id_card_back\"\n"],"metadata":{"id":"M6A5FrwX_GSv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# DOC Detection Model"],"metadata":{"id":"JaiKG-8X5I1_"}},{"cell_type":"code","source":["def detect_and_crop_id_cards(image_path, infer, cconfidence_threshold=0.5):\n","    image = Image.open(image_path).convert(\"RGB\")\n","    image_np = np.array(image)\n","    input_tensor = tf.convert_to_tensor(image_np)\n","    input_tensor = input_tensor[tf.newaxis, ...]\n","\n","    output_dict = infer(input_tensor)\n","\n","    boxes = output_dict['detection_boxes'].numpy()[0]\n","    scores = output_dict['detection_scores'].numpy()[0]\n","\n","    cropped_images = []\n","    height, width, _ = image_np.shape\n","\n","    for box, score in zip(boxes, scores):\n","        if score < confidence_threshold:\n","            continue\n","\n","        y1, x1, y2, x2 = box\n","        x1_px = int(x1 * width)\n","        y1_px = int(y1 * height)\n","        x2_px = int(x2 * width)\n","        y2_px = int(y2 * height)\n","\n","        cropped_img = image.crop((x1_px, y1_px, x2_px, y2_px))\n","        cropped_images.append(cropped_img)\n","\n","    return cropped_images"],"metadata":{"id":"IYT3x9jxZeRC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Loading Models and Preparing Resources / Testing"],"metadata":{"id":"QNR7Joe_5Sum"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LrjEjZGwovaD","colab":{"base_uri":"https://localhost:8080/","height":87,"referenced_widgets":["179bd2edab48495080629a22511193ef","2bc4298f9afa4b7383aedb4a877d9dd5","57e7914511b74e0c921df36edfdbd776","b7853e3028304f099fcc0934375983e0","29bb929a2aa045cfa854a9d39113ae03","99f3780adec94b42b1f5d1dbd512c890","7e95d9fdebc7492e8c920f8d334684d7","8988d8a2c0a14baeaa969ef5e34da97d","8fe32eb4b65040129df546eeda16de77","b706c36930ce4518b8257e264c41ac5b","47d5d92287254be3897963c898547f94"]},"executionInfo":{"status":"ok","timestamp":1757215230204,"user_tz":-60,"elapsed":23981,"user":{"displayName":"Mourad Boutrid","userId":"11848014835914515491"}},"outputId":"5c980903-bbdf-4fb0-9884-3cc33b101b3f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://doctr-static.mindee.com/models?id=v0.4.1/vgg16_bn_r-d108c19c.pt&src=0 to /root/.cache/doctr/models/vgg16_bn_r-d108c19c.pt\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/59214222 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"179bd2edab48495080629a22511193ef"}},"metadata":{}}],"source":["model_path_doctr = '/content/drive/MyDrive/poly-scan-ID/models/doctr_model.pth'\n","config_path_doctr = '/content/drive/MyDrive/poly-scan-ID/models/doctr_model_config.json'\n","\n","predictor = load_model(model_path_doctr, config_path_doctr)\n","\n","# Define the transforms used during training\n","test_transform = transforms.Compose([\n","    transforms.Grayscale(num_output_channels=3),\n","    transforms.Resize((64, 256)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# Load the trained model\n","model_path_classifier = '/content/drive/MyDrive/poly-scan-ID/models/language_classifier.pth'\n","num_classes = 2  # Ar and fr\n","model = LanguageClassifier(num_classes)\n","model.load_state_dict(torch.load(model_path_classifier, map_location=torch.device('cpu')))\n","model.eval()  # Set to evaluation mode\n","\n","\n","paths = [\n","    \"/content/drive/MyDrive/CRAFT-ORC-ID/id-data/id_card.jpg\",\n","    \"/content/drive/MyDrive/CRAFT-ORC-ID/id-data/id_card1.jpg\",\n","    \"/content/drive/MyDrive/CRAFT-ORC-ID/id-data/id_card2.jpg\",\n","    \"/content/drive/MyDrive/CRAFT-ORC-ID/id-data/id_card3.jpg\",\n","    \"/content/drive/MyDrive/CRAFT-ORC-ID/id-data/permi-1.jpg\",\n","    \"/content/drive/MyDrive/CRAFT-ORC-ID/id-data/id-back-1.jpg\",\n","    \"/content/drive/MyDrive/CRAFT-ORC-ID/id-data/id.jpg\",\n","    \"/content/drive/MyDrive/CRAFT-ORC-ID/id-data/p1.jpg\",\n","    \"/content/drive/MyDrive/CRAFT-ORC-ID/id-data/p2.jpg\",\n","    \"/content/drive/MyDrive/CRAFT-ORC-ID/id-data/p4.jpg\",\n","    \"/content/drive/MyDrive/CRAFT-ORC-ID/id-data/pss1.jpg\",\n","    \"/content/drive/MyDrive/CRAFT-ORC-ID/id-data/pss2.jpg\"\n","\n","]\n","\n","import tensorflow as tf\n","\n","# Load the model\n","#mdl = tf.saved_model.load('/content/id-card-detector/model/saved_model')\n","#nfer = mdl.signatures[\"serving_default\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rZhcJCGwodNW"},"outputs":[],"source":["#image, boxes, polys = detect_text(predictor, paths[-2])\n","#show_boxes(image, boxes)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jWHRd11WqYMH"},"outputs":[],"source":["#crops = crop_id_card(image, boxes)\n","#plot_crops(crops, per_row=8, size=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4rplmcmiqYrN"},"outputs":[],"source":["#classification_results = classify_crops(crops)\n","#plot_classification(image, crops, boxes, classification_results)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ob1N5TZ0oRsu"},"outputs":[],"source":["# Run OCR on full image\n","#ocr_results = run_ocr(image)\n","\n","# Display text results\n","#display_ocr_results(ocr_results)\n","\n","# Plot results on image\n","#plot_ocr_results(image, ocr_results)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9qr45F9qoXx-"},"outputs":[],"source":["# Filter OCR results using classification\n","#filtered_texts = filter_ocr_with_classification(ocr_results, image, model, ['Arabic', 'French'], test_transform)\n","\n","# Display results\n","#display_filtered_ocr_results(filtered_texts)\n","\n","# Plot final results\n","#plot_final_results(image, filtered_texts)\n","\n","# Get only French text for final output\n","#french_final_texts = [item for item in filtered_texts if item['classification'] == 'French']\n","#if french_final_texts:\n","    #print(\"\\n=== FINAL FRENCH TEXT ===\")\n","    #for result in french_final_texts:\n","        #print(f\"'{result['text']}'\")\n","#else:\n","    #print(\"\\nNo French text found in final results\")"]},{"cell_type":"markdown","source":["# Face Detection Model (YOLO)"],"metadata":{"id":"l3wpsMHs5rxp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"SAmDOjRvR7Lm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757215230389,"user_tz":-60,"elapsed":126,"user":{"displayName":"Mourad Boutrid","userId":"11848014835914515491"}},"outputId":"5cafc733-9a17-45b1-cbb4-9cd48b219bce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Creating new Ultralytics Settings v0.0.6 file ✅ \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"]}],"source":["from ultralytics import YOLO\n","\n","def box_contains(boxA, boxB):\n","    return (boxA[0] <= boxB[0]) and (boxA[1] <= boxB[1]) and (boxA[2] >= boxB[2]) and (boxA[3] >= boxB[3])\n","\n","def detect_face(image_path, model_path='/content/yolov8x.pt', conf=0.25,\n","                                        edge_threshold=20, max_width_ratio=0.55):\n","    model = YOLO(model_path)\n","    results = model(image_path, conf=conf)\n","\n","    image = cv2.imread(image_path)\n","    if image is None:\n","        raise ValueError(f\"Image not found or unable to read: {image_path}\")\n","\n","    if len(results) == 0 or results[0].boxes is None or len(results[0].boxes) == 0:\n","        return None, None\n","\n","    boxes = results[0].boxes.xyxy.cpu().numpy().astype(int)\n","    img_width = image.shape[1]\n","    img_center_x = img_width / 2\n","\n","    to_remove = set()\n","    n = len(boxes)\n","\n","    for i in range(n):\n","        x0, _, x1, _ = boxes[i]\n","        box_width = x1 - x0\n","\n","        if x0 <= edge_threshold and x1 >= (img_width - edge_threshold):\n","            to_remove.add(i)\n","            continue\n","\n","        if box_width > max_width_ratio * img_width:\n","            to_remove.add(i)\n","            continue\n","\n","        for j in range(n):\n","            if i != j:\n","                if box_contains(boxes[i], boxes[j]):\n","                    to_remove.add(i)\n","\n","    filtered_boxes = [boxes[i] for i in range(n) if i not in to_remove]\n","\n","    if not filtered_boxes:\n","        return None, None\n","\n","    best_face = None\n","    best_crop = None\n","    min_dist = float('inf')\n","\n","    for box in filtered_boxes:\n","        x0, y0, x1, y1 = box\n","        face_center_x = (x0 + x1) / 2\n","\n","        if face_center_x < img_center_x:\n","            dist = img_center_x - face_center_x\n","            if dist < min_dist:\n","                min_dist = dist\n","                best_face = [x0, y0, x1, y1]\n","                best_crop = image[y0:y1, x0:x1]\n","\n","    return best_face, best_crop\n"]},{"cell_type":"code","source":["#def plot_single_face(image_path, face, crop):\n"," #   image = cv2.imread(image_path)\n"," #   image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n"," #   fig, ax = plt.subplots(1, figsize=(12, 10))\n"," #   ax.imshow(image_rgb)\n","\n"," #   if face is not None:\n"," #       x0, y0, x1, y1 = face\n"," #       rect = patches.Rectangle((x0, y0), x1 - x0, y1 - y0,\n"," #                                linewidth=2, edgecolor='lime', facecolor='none')\n"," #       ax.add_patch(rect)\n","\n"," #   plt.axis('off')\n"," #   plt.show()\n","\n"," #   if crop is not None:\n"," #       crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n"," #       plt.figure(figsize=(4, 4))\n"," #      plt.imshow(crop_rgb)\n"," #       plt.title(\"Selected Face\")\n"," #      plt.axis('off')\n"," #       plt.show()\n","\n","#path = paths[5]\n","\n","#face_box, face_crop = detect_face(path)\n","#plot_single_face(path, face_box, face_crop)\n"],"metadata":{"id":"-0bSOneQw0GY"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"179bd2edab48495080629a22511193ef":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2bc4298f9afa4b7383aedb4a877d9dd5","IPY_MODEL_57e7914511b74e0c921df36edfdbd776","IPY_MODEL_b7853e3028304f099fcc0934375983e0"],"layout":"IPY_MODEL_29bb929a2aa045cfa854a9d39113ae03"}},"2bc4298f9afa4b7383aedb4a877d9dd5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_99f3780adec94b42b1f5d1dbd512c890","placeholder":"​","style":"IPY_MODEL_7e95d9fdebc7492e8c920f8d334684d7","value":""}},"57e7914511b74e0c921df36edfdbd776":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8988d8a2c0a14baeaa969ef5e34da97d","max":59214222,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8fe32eb4b65040129df546eeda16de77","value":59214222}},"b7853e3028304f099fcc0934375983e0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b706c36930ce4518b8257e264c41ac5b","placeholder":"​","style":"IPY_MODEL_47d5d92287254be3897963c898547f94","value":" 59214848/? [00:01&lt;00:00, 51588912.48it/s]"}},"29bb929a2aa045cfa854a9d39113ae03":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99f3780adec94b42b1f5d1dbd512c890":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e95d9fdebc7492e8c920f8d334684d7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8988d8a2c0a14baeaa969ef5e34da97d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8fe32eb4b65040129df546eeda16de77":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b706c36930ce4518b8257e264c41ac5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47d5d92287254be3897963c898547f94":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}